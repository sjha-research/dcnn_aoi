{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEe7GkTglctl",
    "outputId": "18915481-c673-4db4-a021-e07169920248"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHoKUrZTXuAP"
   },
   "outputs": [],
   "source": [
    "# rmdir /content/gdrive/MyDrive/Colab Notebooks/generated1kEpochs/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86fCb5pj6qCF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as tfl\n",
    "import scipy.misc\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Lambda, add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "# from resnets_utils import *\n",
    "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ousS3KUl-78T"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2S\n",
    "# from tensorflow.keras.applications import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHFnKHmng1E_",
    "outputId": "aa7b0094-498b-4bb5-eef4-5530d2a17b95"
   },
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkNWX28IKn43",
    "outputId": "b0ffec68-3bc2-496a-b7f1-8d26bd0d8551"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# IMG_SIZE = (160, 160) #for mobileNet v1\n",
    "IMG_SIZE = (224, 224) #for resnet50\n",
    "# directory = \"dataset/\"\n",
    "\n",
    "directory = \"/content/gdrive/MyDrive/Colab Notebooks/CMID/\"\n",
    "\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='training',\n",
    "                                            #  label_mode = tf.float32,\n",
    "                                             seed=42)\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='validation',\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Efq1HQQHHQ2T",
    "outputId": "fbc0d98d-5db8-43ea-eabb-9cf87fd0d5e2"
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPSNzLX4KrjH",
    "outputId": "66044b50-fcb6-4d05-853e-057247aa96e0"
   },
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "# label_names = train_dataset.labels\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRPnc06BTn4f",
    "outputId": "8603ced1-665e-4b20-f8ee-7a529a968af3"
   },
   "outputs": [],
   "source": [
    "# CONVERTING TRAIN DATASET INTO IMAGES AND LABELS\n",
    "i = 0\n",
    "for images, labels in train_dataset: # train_dataset.take(1):\n",
    "  # print(images.shape, labels.shape)\n",
    "  if(i==0):\n",
    "    X_train = images\n",
    "    y_train = labels\n",
    "  else:\n",
    "    X_train = tf.concat([X_train, images], 0)\n",
    "    y_train = tf.concat([y_train, labels], 0)\n",
    "\n",
    "  i = i + 1\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMWuadkFBY7Y"
   },
   "outputs": [],
   "source": [
    "# print(y_train)\n",
    "# print(class_names[labels[len(labels)-1]]) # last image label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNCJVUlrTn4i",
    "outputId": "6921efe7-2178-4b07-9e60-7a1f099d825c"
   },
   "outputs": [],
   "source": [
    "# CONVERTING VALIDATION DATASET INTO IMAGES AND LABELS\n",
    "i = 0\n",
    "for images, labels in validation_dataset: # train_dataset.take(1):\n",
    "  # print(images.shape, labels.shape)\n",
    "  if(i==0):\n",
    "    X_valid = images\n",
    "    y_valid = labels\n",
    "  else:\n",
    "    X_valid = tf.concat([X_valid, images], 0)\n",
    "    y_valid = tf.concat([y_valid, labels], 0)\n",
    "\n",
    "  i = i + 1\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygwGKo90zDpB"
   },
   "outputs": [],
   "source": [
    "# ADDING REAL CMID DATASET (use if above dataset is coming from generated images)\n",
    "# directory = \"/content/gdrive/MyDrive/Colab Notebooks/CMID/\"\n",
    "\n",
    "# train_dataset = image_dataset_from_directory(directory,\n",
    "#                                              shuffle=True,\n",
    "#                                              batch_size=BATCH_SIZE,\n",
    "#                                              image_size=IMG_SIZE,\n",
    "#                                              validation_split=0.2,\n",
    "#                                              subset='training',\n",
    "#                                             #  label_mode = tf.float32,\n",
    "#                                              seed=42)\n",
    "# validation_dataset = image_dataset_from_directory(directory,\n",
    "#                                              shuffle=True,\n",
    "#                                              batch_size=BATCH_SIZE,\n",
    "#                                              image_size=IMG_SIZE,\n",
    "#                                              validation_split=0.2,\n",
    "#                                              subset='validation',\n",
    "#                                              seed=42)\n",
    "# i = 0\n",
    "# for images, labels in train_dataset: # train_dataset.take(1):\n",
    "#   # print(images.shape, labels.shape)\n",
    "#   if(i==0):\n",
    "#     X_train_real = images\n",
    "#     y_train_real = labels\n",
    "#   else:\n",
    "#     X_train_real = tf.concat([X_train_real, images], 0)\n",
    "#     y_train_real = tf.concat([y_train_real, labels], 0)\n",
    "\n",
    "#   i = i + 1\n",
    "# print(X_train_real.shape, y_train_real.shape)\n",
    "# i = 0\n",
    "# for images, labels in validation_dataset: # train_dataset.take(1):\n",
    "#   # print(images.shape, labels.shape)\n",
    "#   if(i==0):\n",
    "#     X_valid_real = images\n",
    "#     y_valid_real = labels\n",
    "#   else:\n",
    "#     X_valid_real = tf.concat([X_valid_real, images], 0)\n",
    "#     y_valid_real = tf.concat([y_valid_real, labels], 0)\n",
    "\n",
    "#   i = i + 1\n",
    "# print(X_valid_real.shape, y_valid_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUJq15kRzESf",
    "outputId": "1822a401-b4cd-4384-bd18-476cfdcaf009"
   },
   "outputs": [],
   "source": [
    "#adding real images 5 times, and then include all the generated image to keep balance of real and generated images \n",
    "# X_train = tf.concat([X_train, X_train_real, X_train_real, X_train_real, X_train_real, X_train_real], 0)\n",
    "# X_valid = tf.concat([X_valid, X_valid_real, X_valid_real, X_valid_real, X_valid_real, X_valid_real], 0)\n",
    "# y_train = tf.concat([y_train, y_train_real, y_train_real, y_train_real, y_train_real, y_train_real], 0)\n",
    "# y_valid = tf.concat([y_valid, y_valid_real, y_valid_real, y_valid_real, y_valid_real, y_valid_real], 0)\n",
    "\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkr_cBKRlknZ",
    "outputId": "bc10e7c2-9292-4415-f49d-4b1ae56bf071"
   },
   "outputs": [],
   "source": [
    "n_H, n_W, n_C = [224, 224, 3]\n",
    "n_classes =1\n",
    "\n",
    "print(n_H, n_W, n_C )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM-q9IiDlvdK"
   },
   "outputs": [],
   "source": [
    "#SPLITTING AND NORMALIZING THE DATASET\n",
    "# X_train, X_valid  = X_train_full[:40000] / 255.0, X_train_full[40000:50000] / 255.0\n",
    "# y_train, y_valid  = y_train_full[:40000] , y_train_full[40000:50000]\n",
    "# X_test = X_test / 255.\n",
    "# for CMID dataset\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "# print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEdS9sxWnp9s",
    "outputId": "fbcf9e7d-7034-4d7f-e326-cd17f2e48ea4"
   },
   "outputs": [],
   "source": [
    "#pretrained models\n",
    "input_shape = (n_H, n_W, n_C)\n",
    "print(input_shape)\n",
    "\n",
    "# base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "#     include_top=False,\n",
    "#     weights='imagenet')\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                                include_top=False, # <== Important!!!!\n",
    "                                                weights='imagenet') # From imageNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_0_meQtjP-N"
   },
   "outputs": [],
   "source": [
    "# TRAIN ONLY THE LAST TWO LAYER OF THE MODEL WITH SOFTMAX\n",
    "# freeze the base model by making it non trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# create the input layer (Same as the imageNetv2 input size)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "# set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "x = base_model(inputs, training=False)\n",
    "# use global avg pooling to summarize the info in each channel\n",
    "x = tfl.GlobalAveragePooling2D()(x)\n",
    "# include dropout with probability of 0.2 to avoid overfitting\n",
    "x = tfl.Dropout(0.2)(x)\n",
    "# Adding one more layer for hybrid model **********************\n",
    "x = tfl.Dense(32, activation='relu', kernel_initializer = tf.keras.initializers.glorot_uniform(seed=0))(x)\n",
    "x = tfl.Dropout(0.2)(x)\n",
    "# use a prediction layer with one neuron (as a binary classifier only needs one), (for more than two class, #neurons = #classes)\n",
    "outputs = tfl.Dense(n_classes, activation='sigmoid', kernel_initializer = tf.keras.initializers.glorot_uniform(seed=0))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "# model_CNN = tf.keras.Model(inputs, outputs=x) #for image gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEVKfrY9DCiT"
   },
   "outputs": [],
   "source": [
    "# # #TRAIN ALL OR SOME OF THE LAYERS OF THE MODEL\n",
    "# base_model.trainable = True\n",
    "# # Let's take a look to see how many layers are in the base model\n",
    "# print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# # # adding regularization\n",
    "# # regularizer = tf.keras.regularizers.l2(l2=0.5)\n",
    "# # for layer in base_model.layers:\n",
    "# #     for attr in ['kernel_regularizer']:\n",
    "# #         if hasattr(layer, attr):\n",
    "# #           setattr(layer, attr, regularizer)\n",
    "\n",
    "# # # Fine-tune from this layer onwards # Train few layers\n",
    "# # fine_tune_at = - 9 #len(base_model.layers)\n",
    "\n",
    "# # # Freeze all the layers before the `fine_tune_at` layer\n",
    "# # for layer in base_model.layers[:fine_tune_at]:\n",
    "# #     layer.trainable = False\n",
    "\n",
    "# # create the input layer (Same as the imageNetv2 input size)\n",
    "# inputs = tf.keras.Input(shape=input_shape)\n",
    "# # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "# x = base_model(inputs, training=True)\n",
    "# # use global avg pooling to summarize the info in each channel\n",
    "# x = tfl.GlobalAveragePooling2D()(x)\n",
    "# # include dropout with probability of 0.2 to avoid overfitting\n",
    "# x = tfl.Dropout(0.2)(x)\n",
    "# # Adding one more layer for hybrid model **********************\n",
    "# x = tfl.Dense(32, activation='relu', kernel_initializer = tf.keras.initializers.glorot_uniform(seed=0))(x)\n",
    "# # use a prediction layer with one neuron (as a binary classifier only needs one), (for more than two class, #neurons = #classes)\n",
    "# outputs = tfl.Dense(n_classes, activation='sigmoid', kernel_initializer = tf.keras.initializers.glorot_uniform(seed=0))(x)\n",
    "\n",
    "# model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "McT-En7SkmXn",
    "outputId": "aafe0a26-74a3-466c-84cd-4aae3f133463"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYswBlgQLv7P"
   },
   "outputs": [],
   "source": [
    "# # REMOVE THE LAST LAYER AND BUILD THE MODEL TO EXTRACT THE FEATURES\n",
    "# inputs_modified = model.layers[0].output\n",
    "# outputs_modified = model.layers[-2].output\n",
    "# model_modified = tf.keras.Model(inputs_modified, outputs_modified)\n",
    "# model_modified.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsKcjrOZmaIP"
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='sparse_categorical_crossentropy',   # for more than two class\n",
    "#               optimizer=tf.keras.optimizers.SGD(lr=0.0001),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# base_learning_rate = 0.0001 #FOR cmid DATASET\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6JJQnUlFsc7"
   },
   "outputs": [],
   "source": [
    "# # training and validation loss/accuracy of DL Models\n",
    "# checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"dl_ml_model.h5\", save_best_only=True, mode='max', monitor='val_accuracy')\n",
    "# history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=10, batch_size=32, callbacks=[checkpoint_cb])\n",
    "# # history = model.fit(train_dataset, validation_data=validation_dataset, epochs=10, batch_size= 32)\n",
    "# best_model = tf.keras.models.load_model(\"dl_ml_model.h5\") # rollback to best model\n",
    "# dl_model_mse_train = best_model.evaluate(X_train, y_train)\n",
    "# dl_model_mse_test = best_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ptm32VCjX1xU"
   },
   "outputs": [],
   "source": [
    "#DATASET PREPARATION FROM TENSOR TO ARRAY LIKE\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "flat_X_train = []\n",
    "flat_y_train = []\n",
    "for i in range(0, len(X_train)):\n",
    "  img_array = X_train[i]\n",
    "  # img_resized = resize(img_array, [224, 224, 3]) #.shape\n",
    "  img_resized = img_array.numpy();\n",
    "  # flat_X_train.append(img_resized.flatten()) #use this line for SVM/RF instead of above line\n",
    "  flat_X_train.append(img_resized)\n",
    "\n",
    "for i in range(0, len(y_train)):\n",
    "  flat_y_train.append(int(y_train[i]))\n",
    "\n",
    "flat_X_train = np.array(flat_X_train)\n",
    "flat_y_train = np.array(flat_y_train)\n",
    "\n",
    "flat_X_valid = []\n",
    "flat_y_valid = []\n",
    "for i in range(0, len(X_valid)):\n",
    "  img_array = X_valid[i]\n",
    "  # img_resized = resize(img_array, [224, 224, 3])\n",
    "  img_resized = img_array.numpy();\n",
    "  # flat_X_valid.append(img_resized.flatten()) #use this line for SVM/RF instead of above line\n",
    "  flat_X_valid.append(img_resized)\n",
    "\n",
    "for i in range(0, len(y_valid)):\n",
    "  flat_y_valid.append(int(y_valid[i]))\n",
    "\n",
    "flat_X_valid = np.array(flat_X_valid)\n",
    "flat_y_valid = np.array(flat_y_valid)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# flat_X_train = scaler.fit_transform(flat_X_train)\n",
    "# flat_X_valid = scaler.fit_transform(flat_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEYGz24ATP-r",
    "outputId": "7a2ccf42-ddb5-40d0-a173-8f8877181649"
   },
   "outputs": [],
   "source": [
    "# CROSS VALIDATION FOR DL AND ML MODELS\n",
    "\n",
    "# CONCATENATE THE DATASET\n",
    "X_main = np.concatenate((flat_X_train, flat_X_valid), axis=0) #flat_X_train.append(flat_X_valid)\n",
    "y_main = np.concatenate((flat_y_train, flat_y_valid), axis=0) #flat_y_train.append()\n",
    "# X = X_main[0:819]; y = y_main[0:819] #train with ALL the datasets\n",
    "X = X_main; y = y_main #train with ALL the datasets\n",
    "# X = X_main[0:400]; y = y_main[0:400] #train with FIRST half of the datasets\n",
    "# X = X_main[400:791]; y = y_main[400:791] #train with SECOND half of the datsets\n",
    "# test_X = X_main[0:100]; test_y = y_main[0:100] #test datasets\n",
    "print('X and y shape= ', X.shape, y.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "fold_no = 1\n",
    "dl_acc =[]; dl_loss =[]; dl_precision =[]; dl_recall=[]; dl_f1_score=[]\n",
    "rf_acc = []; rf_precision =[]; rf_recall=[]; rf_f1_score=[]\n",
    "svm_acc =[]; svm_precision =[]; svm_recall=[]; svm_f1_score=[]\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  # print('train and test shape= ', train.shape, test.shape)\n",
    "  # print('train and test instances= ', train[0], test[0], train, test)\n",
    "  # print('y[test] =', y[test])\n",
    "  # print('X[train=0]=', X[train[0]])\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "  # checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"dl_ml_model.h5\", save_best_only=True, mode='max', monitor='val_accuracy')\n",
    "  history = model.fit(X[train], y[train], validation_data = (X[test], y[test]), batch_size=32, epochs=100, verbose=0) #, callbacks=[checkpoint_cb])\n",
    "  # best_model = tf.keras.models.load_model(\"dl_ml_model.h5\") # rollback to best model\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  # scores_val = best_model.evaluate(X[test], y[test], verbose=0) #evaluate with validation dataset\n",
    "  # scores = best_model.evaluate(test_X, test_y, verbose=0) #evaluate with test dataset\n",
    "\n",
    "  scores = model.evaluate(X[test], y[test], verbose=0) #evaluate with validation dataset\n",
    "\n",
    "  # y_pred_valid = best_model.predict(X[test])\n",
    "  # print(y_pred_valid)\n",
    "  print(f'val_Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}, {model.metrics_names[1]} of {scores[1]*100}%') #validation\n",
    "  # print(f'test_Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%') #test\n",
    "  dl_acc.append(scores[1] * 100)\n",
    "  dl_loss.append(scores[0])\n",
    "\n",
    "  from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "  # predict probabilities for validation set\n",
    "  y_pred_prob = model.predict(X[test])\n",
    "  y_pred_classes = np.zeros((len(y_pred_prob), 1))\n",
    "  for i in range(0, len(y_pred_prob)):\n",
    "    if y_pred_prob[i] < 0.5:\n",
    "      y_pred_classes[i] = 0\n",
    "    else:\n",
    "      y_pred_classes[i] = 1\n",
    "  dl_precision.append(list(precision_score(y[test], y_pred_classes, average=None, zero_division=0)))\n",
    "  dl_recall.append(list(recall_score(y[test], y_pred_classes, average=None, zero_division=0)))\n",
    "  dl_f1_score.append(list(f1_score(y[test], y_pred_classes, average=None, zero_division=0)))\n",
    "\n",
    "  # # REMOVE THE LAST LAYER AND BUILD THE MODEL TO EXTRACT THE FEATURES\n",
    "  # inputs_modified = best_model.layers[0].output\n",
    "  # outputs_modified = best_model.layers[-2].output\n",
    "  # model_modified = tf.keras.Model(inputs_modified, outputs_modified)\n",
    "  # # model_modified.summary()\n",
    "\n",
    "  # # Extracted features from DL models\n",
    "  # X_train_valid = np.concatenate((X[train], X[test]), axis=0)\n",
    "  # y_train_valid = np.concatenate((y[train], y[test]), axis=0)\n",
    "  # y_pred_prob_train_valid = model_modified.predict(X_train_valid, verbose=0)\n",
    "  # y_pred_prob_test = model_modified.predict(test_X, verbose=0)\n",
    "  # print(y_pred_prob_train_valid.shape, y_pred_prob_test.shape)\n",
    "  # # scaled data for classical ML models\n",
    "  # from sklearn.preprocessing import StandardScaler\n",
    "  # scaler = StandardScaler()\n",
    "  # scaled_X_train_valid = scaler.fit_transform(y_pred_prob_train_valid)\n",
    "  # scaled_X_test = scaler.fit_transform(y_pred_prob_test)\n",
    "\n",
    "  # # print('scaled x train and valid=', scaled_X_train.shape, scaled_X_valid.shape, scaled_X_train[0], scaled_X_valid[0])\n",
    "  # # print('y[test]=', y[test])\n",
    "\n",
    "  # # SVM MODEL\n",
    "  # from sklearn import svm\n",
    "  # svm_clf = svm.SVC(kernel='rbf', C=1, gamma=0.0001) # for cross validation\n",
    "  # svm_clf.fit(scaled_X_train_valid, y_train_valid)\n",
    "  # y_pred_test = svm_clf.predict(scaled_X_test)\n",
    "\n",
    "  # from sklearn.metrics import accuracy_score #accuracy for ML models\n",
    "  # svm_acc.append(accuracy_score(test_y, y_pred_test))\n",
    "  # svm_precision.append(list(precision_score(test_y, y_pred_test, average=None)))\n",
    "  # svm_recall.append(list(recall_score(test_y, y_pred_test, average=None)))\n",
    "  # svm_f1_score.append(list(f1_score(test_y, y_pred_test, average=None)))\n",
    "\n",
    "  # # RANDOM FOREST MODEL\n",
    "  # from sklearn.ensemble import RandomForestClassifier\n",
    "  # rf_clf = RandomForestClassifier(n_estimators=32, max_leaf_nodes=16, n_jobs=-1)\n",
    "  # rf_clf.fit(scaled_X_train_valid, y_train_valid)\n",
    "  # y_pred_test = rf_clf.predict(scaled_X_test)\n",
    "\n",
    "  # rf_acc.append(accuracy_score(test_y, y_pred_test))\n",
    "  # rf_precision.append(list(precision_score(test_y, y_pred_test, average=None)))\n",
    "  # rf_recall.append(list(recall_score(test_y, y_pred_test, average=None)))\n",
    "  # rf_f1_score.append(list(f1_score(test_y, y_pred_test, average=None)))\n",
    "  # # print('\\nsvm acc= ', np.mean(svm_acc)*100, '\\nrf acc= ', np.mean(rf_acc)*100) #wrong since it gives cummulative mean\n",
    "  # # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "print('\\nDL acc_mean, and acc_std =', np.mean(dl_acc),', ', np.std(dl_acc))\n",
    "# print('SVM acc_mean, and acc_std =', np.mean(svm_acc)*100,', ', np.std(svm_acc)*100)\n",
    "# print('RF acc_mean, and acc_std =', np.mean(rf_acc)*100,', ', np.std(rf_acc)*100)\n",
    "\n",
    "dl_precision = np.array(dl_precision); dl_precision.reshape(-1,1)\n",
    "# svm_precision = np.array(svm_precision); svm_precision.reshape(-1,1)\n",
    "# rf_precision = np.array(rf_precision); rf_precision.reshape(-1,1)\n",
    "\n",
    "dl_recall = np.array(dl_recall); dl_recall.reshape(-1,1)\n",
    "# svm_recall = np.array(svm_recall); svm_recall.reshape(-1,1)\n",
    "# rf_recall = np.array(rf_recall); rf_recall.reshape(-1,1)\n",
    "\n",
    "dl_f1_score = np.array(dl_f1_score); dl_f1_score.reshape(-1,1)\n",
    "# svm_f1_score = np.array(svm_f1_score); svm_f1_score.reshape(-1,1)\n",
    "# rf_f1_score = np.array(rf_f1_score); rf_f1_score.reshape(-1,1)\n",
    "\n",
    "print('DL precision = ', (np.mean(dl_precision[:,0]) + np.mean(dl_precision[:,1]))/2)\n",
    "# print('SVM precision D and ND', np.mean(svm_precision[:,0]), np.mean(svm_precision[:,1]))\n",
    "# print('RF precision D and ND', np.mean(rf_precision[:,0]), np.mean(rf_precision[:,1]))\n",
    "\n",
    "print('DL recall    = ', (np.mean(dl_recall[:,0]) + np.mean(dl_recall[:,1]))/2)\n",
    "# print('SVM recall D and ND', np.mean(svm_recall[:,0]), np.mean(svm_recall[:,1]))\n",
    "# print('RF recall D and ND', np.mean(rf_recall[:,0]), np.mean(rf_recall[:,1]))\n",
    "\n",
    "print('DL f1_score  = ', (np.mean(dl_f1_score[:,0]) + np.mean(dl_f1_score[:,1]))/2)\n",
    "# print('SVM f1_score D and ND', np.mean(svm_f1_score[:,0]), np.mean(svm_f1_score[:,1]))\n",
    "# print('RF precision D and ND', np.mean(rf_f1_score[:,0]), np.mean(rf_f1_score[:,1]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
