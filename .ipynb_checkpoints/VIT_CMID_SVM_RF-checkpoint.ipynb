{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_weights = models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = models.vit_b_16(weights=pretrained_vit_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Changing Trainable Layers\n",
    "class VitFeatures(nn.Module):\n",
    "    def __init__(self, vit_model, num_trainable_layers=2):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "\n",
    "        # Freeze all parameters first\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze only the last n transformer layers\n",
    "        # num_layers = len(self.vit.encoder.layers)\n",
    "        # for i in range(num_layers - num_trainable_layers, num_layers):\n",
    "        #     for param in self.vit.encoder.layers[i].parameters():\n",
    "        #         param.requires_grad = True\n",
    "        for name, module in self.vit.named_modules():\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Modify the heads structure\n",
    "        self.vit.heads = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features=2),\n",
    "            nn.Linear(in_features=2, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Print trainable layers info\n",
    "        total_params = sum(p.numel() for p in self.vit.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.vit.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit.conv_proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = torch.cat([self.vit.class_token.expand(x.shape[0], -1, -1), x], dim=1)\n",
    "        x = self.vit.encoder(x)\n",
    "        return x[:, 0]  # Return the [CLS] token features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset\n",
    "def create_dataloaders_with_cross_validation(\n",
    "    dataset_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_splits: int = 5,\n",
    "    num_workers: int = os.cpu_count()\n",
    "):\n",
    "    # Use ImageFolder to create the dataset\n",
    "    full_dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "\n",
    "    # Initialize StratifiedKFold for cross-validation\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store train and test data loaders for each split\n",
    "    train_dataloaders = []\n",
    "    test_dataloaders = []\n",
    "\n",
    "    # Get class names\n",
    "    class_names = full_dataset.classes\n",
    "\n",
    "    for train_indices, test_indices in skf.split(range(len(full_dataset)), full_dataset.targets):\n",
    "        # Create train and test datasets for the current split\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "\n",
    "        # Turn images into data loaders\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        train_dataloaders.append(train_dataloader)\n",
    "        test_dataloaders.append(test_dataloader)\n",
    "\n",
    "    return train_dataloaders, test_dataloaders, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half Dataset\n",
    "# NUM_WORKERS = os.cpu_count()\n",
    "# def create_dataloaders_with_cross_validation(\n",
    "#     dataset_dir: str,\n",
    "#     transform: transforms.Compose,\n",
    "#     batch_size: int,\n",
    "#     sampling_ratio: float = 0.5,  # Added sampling ratio parameter\n",
    "#     num_splits: int = 5,\n",
    "#     num_workers: int = NUM_WORKERS\n",
    "# ):\n",
    "#     # Use ImageFolder to create the dataset\n",
    "#     full_dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "    \n",
    "#     # Get indices for each class\n",
    "#     class_indices = {i: [] for i in range(len(full_dataset.classes))}\n",
    "#     for idx, (_, label) in enumerate(full_dataset):\n",
    "#         class_indices[label].append(idx)\n",
    "    \n",
    "#     # Randomly sample indices from each class\n",
    "#     sampled_indices = []\n",
    "#     for class_idx, indices in class_indices.items():\n",
    "#         n_samples = int(len(indices) * sampling_ratio)\n",
    "#         sampled_indices.extend(np.random.choice(indices, size=n_samples, replace=False))\n",
    "    \n",
    "#     # Shuffle the sampled indices\n",
    "#     np.random.shuffle(sampled_indices)\n",
    "    \n",
    "#     # Create a subset of the dataset with only sampled indices\n",
    "#     sampled_dataset = torch.utils.data.Subset(full_dataset, sampled_indices)\n",
    "#     sampled_targets = [full_dataset.targets[i] for i in sampled_indices]\n",
    "    \n",
    "#     print(f\"Original dataset size: {len(full_dataset)}\")\n",
    "#     print(f\"Sampled dataset size: {len(sampled_dataset)} ({sampling_ratio*100}%)\")\n",
    "    \n",
    "#     # Initialize StratifiedKFold for cross-validation\n",
    "#     skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "#     # Initialize lists to store train and test data loaders for each split\n",
    "#     train_dataloaders = []\n",
    "#     test_dataloaders = []\n",
    "#     # Get class names\n",
    "#     class_names = full_dataset.classes\n",
    "    \n",
    "#     for train_indices, test_indices in skf.split(range(len(sampled_dataset)), sampled_targets):\n",
    "#         # Create train and test datasets for the current split\n",
    "#         train_dataset = torch.utils.data.Subset(sampled_dataset, train_indices)\n",
    "#         test_dataset = torch.utils.data.Subset(sampled_dataset, test_indices)\n",
    "#         # Turn images into data loaders\n",
    "#         train_dataloader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "#         test_dataloader = DataLoader(\n",
    "#             test_dataset,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "#         train_dataloaders.append(train_dataloader)\n",
    "#         test_dataloaders.append(test_dataloader)\n",
    "#     return train_dataloaders, test_dataloaders, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Extract features from the ViT model using the full forward pass.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            features_batch = model(X)\n",
    "            features.append(features_batch.cpu().numpy())\n",
    "            labels.append(y.numpy())\n",
    "\n",
    "    return np.vstack(features), np.hstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate all metrics for a given set of predictions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'accuracy': (y_pred == y_true).mean(),\n",
    "        'precision': precision_score(y_true, y_pred, average=\"weighted\"),\n",
    "        'recall': recall_score(y_true, y_pred, average=\"weighted\"),\n",
    "        'f1': f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "def train_svm_with_cross_validation(train_dataloaders, test_dataloaders, vit_model, device):\n",
    "    all_results = []\n",
    "\n",
    "    # Create feature extractor with only last 2 layers trainable\n",
    "    feature_extractor = VitFeatures(vit_model, num_trainable_layers=2).to(device)\n",
    "\n",
    "    for split in range(len(train_dataloaders)):\n",
    "        print(f\"\\nTraining on Split {split+1}\")\n",
    "\n",
    "        # Extract features\n",
    "        train_features, train_labels = extract_features(train_dataloaders[split], feature_extractor, device)\n",
    "        test_features, test_labels = extract_features(test_dataloaders[split], feature_extractor, device)\n",
    "\n",
    "        # Train SVM\n",
    "        svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma=0.0001, probability=True))\n",
    "        svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "        # Get predictions\n",
    "        train_pred = svm_classifier.predict(train_features)\n",
    "        test_pred = svm_classifier.predict(test_features)\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_metrics = evaluate_metrics(train_labels, train_pred)\n",
    "        test_metrics = evaluate_metrics(test_labels, test_pred)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Split {split+1} Results:\")\n",
    "        print(f\"Train Metrics - Accuracy: {train_metrics['accuracy']*100:.2f}%, \"\n",
    "              f\"Precision: {train_metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {train_metrics['recall']:.4f}, \"\n",
    "              f\"F1: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"Test Metrics  - Accuracy: {test_metrics['accuracy']*100:.2f}%, \"\n",
    "              f\"Precision: {test_metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {test_metrics['recall']:.4f}, \"\n",
    "              f\"F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'split': split + 1,\n",
    "            'train': train_metrics,\n",
    "            'test': test_metrics\n",
    "        })\n",
    "\n",
    "    # Print average results across all splits\n",
    "    avg_train_acc = np.mean([r['train']['accuracy'] for r in all_results])\n",
    "    avg_test_acc = np.mean([r['test']['accuracy'] for r in all_results])\n",
    "    print(f\"\\nAverage Results Across All Splits:\")\n",
    "    print(f\"Average Train Accuracy: {avg_train_acc*100:.2f}%\")\n",
    "    print(f\"Average Test Accuracy: {avg_test_acc*100:.2f}%\")\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF Model\n",
    "# def train_svm_with_cross_validation(train_dataloaders, test_dataloaders, vit_model, device, n_estimators=32):\n",
    "#     all_results = []\n",
    "\n",
    "#     # Create feature extractor with only last 2 layers trainable\n",
    "#     feature_extractor = VitFeatures(vit_model).to(device)\n",
    "\n",
    "#     for split in range(len(train_dataloaders)):\n",
    "#         print(f\"\\nTraining on Split {split+1}\")\n",
    "\n",
    "#         # Extract features\n",
    "#         train_features, train_labels = extract_features(train_dataloaders[split], feature_extractor, device)\n",
    "#         test_features, test_labels = extract_features(test_dataloaders[split], feature_extractor, device)\n",
    "\n",
    "#         # Train Random Forest \n",
    "#         rf_classifier = RandomForestClassifier(n_estimators=32, max_leaf_nodes=16, n_jobs=-1, random_state=42)  \n",
    "#         rf_classifier.fit(train_features, train_labels)\n",
    "\n",
    "#         # Get predictions\n",
    "#         train_pred = rf_classifier.predict(train_features)\n",
    "#         test_pred = rf_classifier.predict(test_features)\n",
    "\n",
    "#         # Calculate metrics\n",
    "#         train_metrics = evaluate_metrics(train_labels, train_pred)\n",
    "#         test_metrics = evaluate_metrics(test_labels, test_pred)\n",
    "\n",
    "#         # Print results\n",
    "#         print(f\"Split {split+1} Results:\")\n",
    "#         print(f\"Train Metrics - Accuracy: {train_metrics['accuracy']*100:.2f}%, \"\n",
    "#               f\"Precision: {train_metrics['precision']:.4f}, \"\n",
    "#               f\"Recall: {train_metrics['recall']:.4f}, \"\n",
    "#               f\"F1: {train_metrics['f1']:.4f}\")\n",
    "#         print(f\"Test Metrics  - Accuracy: {test_metrics['accuracy']*100:.2f}%, \"\n",
    "#               f\"Precision: {test_metrics['precision']:.4f}, \"\n",
    "#               f\"Recall: {test_metrics['recall']:.4f}, \"\n",
    "#               f\"F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "#         # Store results\n",
    "#         all_results.append({\n",
    "#             'split': split + 1,\n",
    "#             'train': train_metrics,\n",
    "#             'test': test_metrics\n",
    "#         })\n",
    "\n",
    "#     # Print average results across all splits\n",
    "#     avg_train_acc = np.mean([r['train']['accuracy'] for r in all_results])\n",
    "#     avg_test_acc = np.mean([r['test']['accuracy'] for r in all_results])\n",
    "#     print(f\"\\nAverage Results Across All Splits:\")\n",
    "#     print(f\"Average Train Accuracy: {avg_train_acc*100:.2f}%\")\n",
    "#     print(f\"Average Test Accuracy: {avg_test_acc*100:.2f}%\")\n",
    "\n",
    "#     return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/kaggle/input/cmid-dataset/CMID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders_with_cross_validation(\n",
    "    dataset_dir=dataset_dir,\n",
    "    transform=pretrained_vit_transforms,\n",
    "    batch_size=32,\n",
    "    num_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half Dataset\n",
    "# train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders_with_cross_validation(\n",
    "#     dataset_dir=dataset_dir,\n",
    "#     transform=pretrained_vit_transforms,\n",
    "#     batch_size=32,\n",
    "#     sampling_ratio=0.5,  # Use 50% of the data\n",
    "#     num_splits=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = train_svm_with_cross_validation(\n",
    "    train_dataloader_pretrained,\n",
    "    test_dataloader_pretrained,\n",
    "    pretrained_vit,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_results = train_svm_with_cross_validation(\n",
    "#     train_dataloader_pretrained,\n",
    "#     test_dataloader_pretrained,\n",
    "#     pretrained_vit,\n",
    "#     device,\n",
    "#     n_estimators=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4137925,
     "sourceId": 7163592,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
