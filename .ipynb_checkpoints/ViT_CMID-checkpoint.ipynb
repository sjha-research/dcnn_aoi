{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7163592,"sourceType":"datasetVersion","datasetId":4137925}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# First cell - Imports\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchinfo import summary\nfrom tqdm.auto import tqdm\nimport os\nfrom typing import Dict, List, Tuple\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For Change the trainablr layer\n# Second cell - Custom ViT Model\nclass CustomViT(nn.Module):\n    def __init__(self, base_model):\n        super().__init__()\n        self.vit = base_model\n        \n        # Freeze all parameters initially\n        for parameter in self.vit.parameters():\n            parameter.requires_grad = False\n            \n        # Make only LayerNorm layers trainable\n        for name, module in self.vit.named_modules():\n            if isinstance(module, nn.LayerNorm):\n                for param in module.parameters():\n                    param.requires_grad = True\n        \n        # Modify the heads structure\n        self.vit.heads = nn.Sequential(\n            nn.Linear(in_features=768, out_features=2),\n            nn.Linear(in_features=2, out_features=1),\n            nn.Sigmoid()\n        )\n        \n        # Make the heads trainable\n        for param in self.vit.heads.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        return self.vit(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Third cell - Setup functions\ndef setup_model(device):\n    pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    base_model = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n    model = CustomViT(base_model).to(device)\n    \n    # Verify trainable layers\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(f\"Trainable layer: {name}\")\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fourth cell - Training and testing functions\ndef train_step_binary(model: torch.nn.Module,\n                     dataloader: torch.utils.data.DataLoader,\n                     loss_fn: torch.nn.Module,\n                     optimizer: torch.optim.Optimizer,\n                     device: torch.device) -> Tuple[float, float]:\n    model.train()\n    train_loss, train_acc = 0, 0\n    \n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device).float()\n        \n        # Forward pass\n        y_pred = model(X)\n        y_pred = y_pred.squeeze()\n        \n        # Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item()\n        \n        # Optimizer zero grad\n        optimizer.zero_grad()\n        \n        # Loss backward\n        loss.backward()\n        \n        # Optimizer step\n        optimizer.step()\n        \n        # Calculate accuracy\n        y_pred_class = (y_pred > 0.5).float()\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n    \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\ndef test_step_binary(model: torch.nn.Module,\n                    dataloader: torch.utils.data.DataLoader,\n                    loss_fn: torch.nn.Module,\n                    device: torch.device) -> Tuple[float, float]:\n    model.eval()\n    test_loss, test_acc = 0, 0\n    \n    with torch.inference_mode():\n        for batch, (X, y) in enumerate(dataloader):\n            X, y = X.to(device), y.to(device).float()\n            \n            # Forward pass\n            test_pred = model(X)\n            test_pred = test_pred.squeeze()\n            \n            # Calculate loss\n            loss = loss_fn(test_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            test_pred_class = (test_pred > 0.5).float()\n            test_acc += (test_pred_class == y).sum().item()/len(test_pred)\n            \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For half Datasets\nNUM_WORKERS = os.cpu_count()\ndef create_dataloaders_with_cross_validation(\n    dataset_dir: str,\n    transform: transforms.Compose,\n    batch_size: int,\n    sampling_ratio: float = 0.5,  # Added sampling ratio parameter\n    num_splits: int = 5,\n    num_workers: int = NUM_WORKERS\n):\n    # Use ImageFolder to create the dataset\n    full_dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n    \n    # Get indices for each class\n    class_indices = {i: [] for i in range(len(full_dataset.classes))}\n    for idx, (_, label) in enumerate(full_dataset):\n        class_indices[label].append(idx)\n    \n    # Randomly sample indices from each class\n    sampled_indices = []\n    for class_idx, indices in class_indices.items():\n        n_samples = int(len(indices) * sampling_ratio)\n        sampled_indices.extend(np.random.choice(indices, size=n_samples, replace=False))\n    \n    # Shuffle the sampled indices\n    np.random.shuffle(sampled_indices)\n    \n    # Create a subset of the dataset with only sampled indices\n    sampled_dataset = torch.utils.data.Subset(full_dataset, sampled_indices)\n    sampled_targets = [full_dataset.targets[i] for i in sampled_indices]\n    \n    print(f\"Original dataset size: {len(full_dataset)}\")\n    print(f\"Sampled dataset size: {len(sampled_dataset)} ({sampling_ratio*100}%)\")\n    \n    # Initialize StratifiedKFold for cross-validation\n    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    # Initialize lists to store train and test data loaders for each split\n    train_dataloaders = []\n    test_dataloaders = []\n    # Get class names\n    class_names = full_dataset.classes\n    \n    for train_indices, test_indices in skf.split(range(len(sampled_dataset)), sampled_targets):\n        # Create train and test datasets for the current split\n        train_dataset = torch.utils.data.Subset(sampled_dataset, train_indices)\n        test_dataset = torch.utils.data.Subset(sampled_dataset, test_indices)\n        # Turn images into data loaders\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n        test_dataloader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n        train_dataloaders.append(train_dataloader)\n        test_dataloaders.append(test_dataloader)\n    return train_dataloaders, test_dataloaders, class_names\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For Full Dataset\n# Fifth cell - Dataloader and training functions\n# def create_dataloaders_with_cross_validation(\n#     dataset_dir: str,\n#     transform: transforms.Compose,\n#     batch_size: int,\n#     num_splits: int = 5,\n#     num_workers: int = os.cpu_count()\n# ):\n#     full_dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n#     skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n#     train_dataloaders = []\n#     test_dataloaders = []\n#     class_names = full_dataset.classes\n    \n#     for train_indices, test_indices in skf.split(full_dataset.imgs, full_dataset.targets):\n#         train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n#         test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n        \n#         train_dataloader = DataLoader(\n#             train_dataset,\n#             batch_size=batch_size,\n#             shuffle=True,\n#             num_workers=num_workers,\n#             pin_memory=True,\n#         )\n#         test_dataloader = DataLoader(\n#             test_dataset,\n#             batch_size=batch_size,\n#             shuffle=False,\n#             num_workers=num_workers,\n#             pin_memory=True,\n#         )\n        \n#         train_dataloaders.append(train_dataloader)\n#         test_dataloaders.append(test_dataloader)\n    \n#     return train_dataloaders, test_dataloaders, class_names\n\ndef train_with_cross_validation(model, train_dataloaders, test_dataloaders, \n                              optimizer, loss_fn, epochs, device,\n                              train_step_fn, test_step_fn):\n    all_results = []\n    \n    for split in range(len(train_dataloaders)):\n        results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n        \n        for epoch in tqdm(range(epochs)):\n            train_loss, train_acc = train_step_fn(\n                model=model,\n                dataloader=train_dataloaders[split],\n                loss_fn=loss_fn,\n                optimizer=optimizer,\n                device=device\n            )\n            \n            test_loss, test_acc = test_step_fn(\n                model=model,\n                dataloader=test_dataloaders[split],\n                loss_fn=loss_fn,\n                device=device\n            )\n            \n            print(\n                f\"Split: {split+1} | Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.4f} | \"\n                f\"train_acc: {train_acc:.4f} | \"\n                f\"test_loss: {test_loss:.4f} | \"\n                f\"test_acc: {test_acc:.4f}\"\n            )\n            \n            results[\"train_loss\"].append(train_loss)\n            results[\"train_acc\"].append(train_acc)\n            results[\"test_loss\"].append(test_loss)\n            results[\"test_acc\"].append(test_acc)\n            \n        all_results.append(results)\n        \n    return all_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sixth cell - Setup and run training\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set dataset directory\ndataset_dir = '/kaggle/input/cmid-dataset/CMID'  # Your dataset path\n\n# Create model\nmodel = setup_model(device)\n\n# Get transforms\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\ntransforms = pretrained_vit_weights.transforms()\n\n# Create dataloaders\ntrain_dataloaders, test_dataloaders, class_names = create_dataloaders_with_cross_validation(\n    dataset_dir=dataset_dir,\n    transform=transforms,\n    batch_size=32,\n    num_splits=5,\n    sampling_ratio=0.5\n)\n\n# Setup loss and optimizer\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(\n    [p for p in model.parameters() if p.requires_grad],\n    lr=1e-3\n)\n\n# Train with cross validation\nresults = train_with_cross_validation(\n    model=model,\n    train_dataloaders=train_dataloaders,\n    test_dataloaders=test_dataloaders,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    epochs=100,\n    device=device,\n    train_step_fn=train_step_binary,\n    test_step_fn=test_step_binary\n)\n\n# Print model summary\nsummary(model=model,\n        input_size=(32, 3, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}